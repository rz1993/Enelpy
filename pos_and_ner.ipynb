{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word Disambiguation\n",
    "\n",
    "When building a model to extract information from text, it often helps to encode some kind of meaning for each word as features for our model. A basic way to do is known as **part-of-speech** (or POS) tagging. This low level task uses various features about the text to frame a classification problem of labeling each word with the correct part of speech in a given document. Why do we do this? The intuition is that having additional information about how a word is being used will allow a model to perform better inference on some higher level task.\n",
    "<br><br>\n",
    "Formally, given an unlabeled sequence, POS-tagging is the task of labeling each token in that sequence with its part-of-speech. Here is an example of doing so using **`spaCy`**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'PRON', 'PRP'), ('am', 'VERB', 'VBP'), ('a', 'DET', 'DT'), ('happy', 'ADJ', 'JJ'), ('boy', 'NOUN', 'NN'), ('.', 'PUNCT', '.')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "model = spacy.load('en')\n",
    "doc = model('I am a happy boy.')\n",
    "labeled_tokens = [(token.text, token.pos_, token.tag_) for token in doc]\n",
    "print(\", \".join(map(str, labeled_tokens)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implementing our own POS-Tagger\n",
    "\n",
    "While various methods exist of labeling a sequence of tokens, the most popular is the Hidden Markov Model (HMM). On a very high level, HMMs are a sort of supervised model that labels sequences by using a learned probability model and some markov assumptions. Given a sequence of objects, **x**, an HMM assumes the following:\n",
    "> 1. Each object in **x_i** is generated as a result of some hidden state **z_i**.\n",
    "> 2. There is a sequence of *hidden* states responsible for generating the observed sequence.\n",
    "> 3. There is a conditional probabilistic distribution P(x|z) that governs this generation process.\n",
    "> 4. The sequence of hidden states is generated by a conditional distribution as well; given the current timestep *i* and some window size *j*, the state **z_i** is only dependent on **z_i-j** to **z_i-1**. This is what is known as a Markov assumption.\n",
    "\n",
    "Here, our words (tokens) are our **x**, while their associated part-of-speech tags are our hidden states, **z**. Thus, we must learn two distributions in order to have a functioning HMM: P(x|z) and P(z_i|z_i-j:z_i-1). Since **x** is a token, we can learn the conditional distribution P(x|z) for a specific **x_i** and **z_i** by using the probabilistic rule that: P(x|z) = P(x, z)/P(z). While we don't have the true probabilities P(x, z) and P(z), we can approximate both empirically using maximum likelihood estimation. Our P(x|z) then becomes a ratio of counts.\n",
    "\n",
    "We can do the same for the conditional distribution of z. For simplification, we will assume that each pos-tag only depends on the tag of the previous word. Here is a model class that will fit these two distributions given a labeled sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "\n",
    "# A callable for nesting a defaultdict inside another\n",
    "defaultdict_int = lambda n: lambda : defaultdict(lambda : n)\n",
    "\n",
    "START_SENTINEL = 'START'\n",
    "\n",
    "class HmmDistribution:\n",
    "    def __init__(self, seq=None, lp_smoother=1):\n",
    "        self.token_counts = None\n",
    "        self.label_counts = None\n",
    "        self._fitted = False\n",
    "        self._lp_smoother = lp_smoother\n",
    "        self.N = 0\n",
    "        if seq is not None:\n",
    "            self.fit(seq)\n",
    "    \n",
    "    def is_fit(self):\n",
    "        return self._fitted\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.token_counts = None\n",
    "        self.label_counts = None\n",
    "        self._fitted = False\n",
    "        self.N = 0\n",
    "    \n",
    "    def fit(self, seq, reset=False):\n",
    "        if not isinstance(seq[0], tuple):\n",
    "            raise Exception('Data should be a sequence of (token, label) tuples.')\n",
    "        \n",
    "        if reset:\n",
    "            self._reset()\n",
    "        elif self.is_fit():\n",
    "            token_counts = self.token_counts\n",
    "            label_counts = self.label_counts\n",
    "        else:\n",
    "            token_counts = defaultdict(defaultdict_int(self._lp_smoother))\n",
    "            label_counts = defaultdict(defaultdict_int(self._lp_smoother))\n",
    "        \n",
    "        prev_label = START_SENTINEL\n",
    "        for token, label in seq:\n",
    "            token_counts[token][label] += 1\n",
    "            label_counts[label][prev_label] += 1\n",
    "            self.N += 1\n",
    "            \n",
    "            prev_label = label\n",
    "    \n",
    "        if not self.is_fit():\n",
    "            self.token_counts = token_counts\n",
    "            self.label_counts = label_counts\n",
    "            self._fitted = True\n",
    "    \n",
    "    def get_marginal_token(self, token):\n",
    "        return self.get_token_count(token) / float(self.N)\n",
    "    \n",
    "    def get_marginal_label(self, label):\n",
    "        return self.get_label_count(label) / float(self.N)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return list(self.label_counts.keys())\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return list(self.token_counts.keys())\n",
    "    \n",
    "    def get_token_count(self, token):\n",
    "        return sum(self.token_counts[token].values())\n",
    "    \n",
    "    def get_label_count(self, label):\n",
    "        return sum(self.label_counts[label].values())\n",
    "    \n",
    "    def get_top_token_labels(self, token, n=5):\n",
    "        return sorted(self.token_counts[token].items(), key=lambda pair: pair[0], reverse=True)[:n]\n",
    "    \n",
    "    def get_joint(self, token, label):\n",
    "        return self.token_counts[token][label] / float(self.N)\n",
    "    \n",
    "    def get_conditional(self, token, label):\n",
    "        return self.get_joint(token, label) / self.get_token_count(token)\n",
    "    \n",
    "    def get_conditional_label(self, label, cond_label):\n",
    "        return self.label_counts[label][cond_label] / self.get_label_count(cond_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Word', 'POS'),\n",
       " ('Thousands', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('demonstrators', 'NNS'),\n",
       " ('have', 'VBP')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here we can test our class against some POS-labeled text data from Kaggle.\n",
    "'''\n",
    "import csv\n",
    "\n",
    "url = 'https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/downloads/ner_dataset.csv'\n",
    "\n",
    "csv_path = 'data/ner_dataset.csv'\n",
    "with open(csv_path, 'r') as csv_file:\n",
    "    sequence = []\n",
    "    line = csv_file.readline()\n",
    "    \n",
    "    # There are some bytes in the csv that cannot be decoded with the standard 'utf-8'\n",
    "    while line:\n",
    "        try:\n",
    "            row = line.split(',')\n",
    "            if len(row) == 4:\n",
    "                labeled_pair = (row[1], row[2])\n",
    "            sequence.append(labeled_pair)\n",
    "            line = csv_file.readline()\n",
    "        except UnicodeDecodeError as e:\n",
    "            continue\n",
    "\n",
    "sequence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb count: 15970\n",
      "Noun count: 79415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRP', 141), ('NNP', 43)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = HmmDistribution(sequence)\n",
    "\n",
    "print('Verb count: %i' % dist.get_label_count('VBP'))\n",
    "print('Noun count: %i' % dist.get_label_count('NNS'))\n",
    "\n",
    "dist.get_top_token_labels('I')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tagging using HMM Inference\n",
    "\n",
    "Now that we have a model capable of fitting conditional distributions to a labeled sequence, we can use it to calculate the probability of a sequence of tokens and their hidden POS tags. This is simplified by the markov assumption that HMMs make: the transition from one state to the next is determined completely by fixed size window of prior states (in our case 1). Assuming the window of dependence is 1:\n",
    "\n",
    "> At a given step the probability of observing a word *w* and tag *t_i* is P(w|t_i)P(t_i|t_i-1)\n",
    "> The joint probability of a sequence of tags **t** can be factorized using the markov assumption:\n",
    ">> P(**t**) = P(t_0) * product from i=1 to T of P(t_i|t_i-1)\n",
    "> The joint probability for a labeled sequence is then:\n",
    ">> P(**w**, **t**) = P(w_0|t_0) * P(t_0) * product from i=1 to T of P(t_i|t_i-1)P(w_i|t_i)\n",
    "\n",
    "The code for this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability is: -66.009707223\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def joint_log_probability(model, seq):\n",
    "    '''\n",
    "    Calculate the log joint likelihood of a labeled sequence. Using log prevents underflow.\n",
    "    '''\n",
    "    N = model.N\n",
    "    \n",
    "    # Get the joint probability for the first step in the sequence\n",
    "    word, tag = seq[0]\n",
    "    prob = math.log(model.get_marginal_label(tag) * model.get_conditional(word, tag))\n",
    "    \n",
    "    # Iteratively compute the likelihood through the remaining sequence\n",
    "    for w, t in seq[1:]:\n",
    "        prob += math.log(model.get_conditional_label(t, tag) * model.get_conditional(w, t))\n",
    "        tag = t\n",
    "    \n",
    "    return prob\n",
    "\n",
    "sentence = \"I have a dream\"\n",
    "tags = \"NNP VBP DT NN\"\n",
    "labeled_seq = list(zip(sentence.split(' '), tags.split(' ')))\n",
    "print(\"Joint probability is: {0:.9f}\".format(joint_log_probability(dist, labeled_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The Viterbi Algorithm\n",
    "Now that we have a method of computing joint log probabilities for a labeled sequence, we can compare labels for a word sequence and find the optimal one... right?!\n",
    "\n",
    "Unfortunately, this is intractable. If we had a sequence of *S* words, and a vocabulary size of *V*, the space we would be comparing over is *V^S*! Luckily, there is a better way to do this without searching over all permutations and it makes use, again, of the Markov assumption.\n",
    "\n",
    "Observing that each hidden state depends only on the state before it for a *one-off* HMM, the transition from, say, 'NNP' to 'VBP' is the same, regardless of what tag preceded 'NNP'. This means that if we are computing the likelihood of a tag sequence that ends with (...,'NNP', 'VBP'), we would be wasting precious time recomputing the probability for all sequences (..., 'NNP') and then multiplying by P('VBP'|'NNP') for each; instead for any tag, e.g. 'VBP', we can see that the sequence with the highest probability will consist of the subsequence ending at X appended with 'VBP', whereby x is the tag that yields the largest probability P(t_1...x) * P('VBP'|x). Notice that in order to find the best tag x from all possible tags, we must do the same thing we did with 'VBP', that is find the sequence ending at X with the largest probability, and then compare amongst all x's. Thus we have a dynamic programming solution that requires evaluating only two steps of the sequence at each step.\n",
    "> 1. At step 1, store a table of pointers for each tag to their joint probability with generating the first word.\n",
    "> 2. For each subsequent step i > 1, we compute the max likelihood for any sequence ending at step i with some tag t_i and storing this probability (log probability in practice) with a pointer to tag t_i.\n",
    "> 3. This max likelihood is calculated for each t_i, over all possible previous tags, t_i-1 (which will have their largest sequence probabilities stored).\n",
    "> 4. We will also maintain a matrix of pointers NxT, where the optimal previous tag for each current tag is stored so that we can backtrack and recover the optimal tag sequence at the end of the algorithm.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # For fast vector computations\n",
    "\n",
    "\n",
    "class ViterbiTagger:\n",
    "    def __init__(self, seq=None):\n",
    "        self.model = HmmDistribution()\n",
    "        self.seq = seq\n",
    "        self.tags = None\n",
    "        self.tokens = None\n",
    "        self.tags_to_index = None\n",
    "        if seq is not None:\n",
    "            self.fit(seq)\n",
    "    \n",
    "    def fit(self, seq):\n",
    "        self.model.fit(seq)\n",
    "        self.tags = self.model.get_labels()\n",
    "        self.tags_to_index = dict(((self.tags[i], i) for i in range(len(self.tags))))\n",
    "        self.tokens = self.model.get_tokens()\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        priors = [math.log(self.model.get_joint(seq[0], tag)) for tag in self.tags]\n",
    "        pointers = []\n",
    "        \n",
    "        for i, token in enumerate(seq):\n",
    "            new_priors = []\n",
    "            current_pointers = []\n",
    "            \n",
    "            for j, t_j in enumerate(self.tags):\n",
    "                max_prob, max_pointer = 0, 0\n",
    "                for k, t_k in enumerate(self.tags):\n",
    "                    prob = math.log(self.model.get_conditional_label(t_j, t_k)) + \\\n",
    "                           priors[k]\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_pointer = k\n",
    "                new_priors.append(max_prob + math.log(self.model.get_conditional(token, t_j)))\n",
    "                current_pointers.append(max_pointer)\n",
    "            \n",
    "            priors = new_priors\n",
    "            pointers.append(current_pointers)\n",
    "        \n",
    "        return priors, pointers\n",
    "    \n",
    "    def backtrack_tags(self, probs, pointers):\n",
    "        T = len(pointers)\n",
    "        max_prob, max_index = 0, 0\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob > max_prob:\n",
    "                max_probs = prob\n",
    "                max_index = i\n",
    "        \n",
    "        tags = [self.tags[max_index]]\n",
    "        for t in range(T-2, -1, -1):\n",
    "            max_index = pointers[t][max_index]\n",
    "            tags.append(self.tags[max_index])\n",
    "        \n",
    "        return tags[::-1]\n",
    "    \n",
    "    def tag_sequence(self, tokens):\n",
    "        probs, pointers = self.viterbi(tokens)\n",
    "        tags = self.backtrack_tags(probs, pointers)\n",
    "        \n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07\n"
     ]
    }
   ],
   "source": [
    "tagger = ViterbiTagger()\n",
    "\n",
    "tagger.fit(sequence)\n",
    "tokens_train, tags_train = zip(*sequence[:100])\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    return float(correct) / len(y_true)\n",
    "\n",
    "tags_predict = tagger.tag_sequence(tokens_train)\n",
    "print(accuracy(tags_train, tags_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ecbm4040]",
   "language": "python",
   "name": "conda-env-ecbm4040-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
